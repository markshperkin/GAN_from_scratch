{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e214989c-25c7-41ae-9e7b-e491b78140bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11c30121-49fe-4bf7-bca5-0124060b3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20906a7-8b56-483c-831d-1a6b997d0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Logger # used for visualize the training process and time store it in tensor board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b1a662-65c1-4030-be63-9099a5226c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaoding the MNIST data set who will be mapped in tange of (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a2408d-9bfa-443b-b8b6-2e49964798e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_data():\n",
    "    compose = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # using grayscale, not rgb like the tutorial\n",
    "        ])\n",
    "    out_dir = './dataset'\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "# Load data\n",
    "data = mnist_data()\n",
    "# Create loader with data, so that we can iterate over it\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True)\n",
    "# Num batches\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cfd8111-c92f-4624-b63a-177c9042c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the Discriminator\n",
    "# this network will take a flattened (one dimentional array) image as its input\n",
    "# return the probabiliry of it belonging to the readl data set or the synthetic dataset, the one created by the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9b03b3-46a2-4e98-bafd-643786160792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer discriminative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        n_features = 784\n",
    "        n_out = 1\n",
    "        \n",
    "        self.hidden0 = nn.Sequential( \n",
    "            nn.Linear(n_features, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(256, n_out),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "discriminator = DiscriminatorNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6291c465-7ec2-4033-80f8-24808ce8cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to allow us to convert a flattened image into two dimentional representation\n",
    "# define a function to allow us to convert a two dimentional image into flatterned image or one dimentional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62ca0337-1489-40f6-a281-4f4ed215da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_vectors(images):\n",
    "    return images.view(images.size(0), 784)\n",
    "\n",
    "def vectors_to_images(vectors):\n",
    "    return vectors.view(vectors.size(0), 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0afef06c-8168-481b-a93a-51f0de0f7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the generator network\n",
    "# this model will have three hidden layers, with Leaky-ReLU activation function who has a nonlinearity feature\n",
    "# the output layer will have a TanH activation function which maps the resulting values into (-1,1) range so it will match the range of the MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d0b390-fc38-4097-930e-46f7fc4da7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer generative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        n_features = 100\n",
    "        n_out = 784\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(1024, n_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "generator = GeneratorNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ae732b-cbbd-420d-a2cb-01e985b11975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function noise that allows us to create the random noise\n",
    "# the random noise will be sampled from a normal distribution with mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3706fe9-ddab-4db7-9c67-905053f69ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(size):\n",
    "    '''\n",
    "    Generates a 1-d vector of gaussian sampled random values\n",
    "    '''\n",
    "    n = Variable(torch.randn(size, 100))\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e223693-7b64-43ca-8bbb-ee5f8c5e6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use Adam optimazer algorithm for both NN with learning rate of 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49dbabb3-4854-44b3-8396-38cd8d229f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "107cdfd7-7c1a-48f6-ba40-b53a77d4a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss function we will use for this task is BInary Cross Entopy Loss\n",
    "# it resembles the log-loss for both the generator and discriminator \n",
    "# we will be taking the average of the loss calculated for each minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b00b6ef5-e8bb-4ca6-9de5-7995cc7479c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef510310-e2a9-423e-bbbb-fdbe0dd235f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a target for the images where one is for the real images and zero for the fake images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "579c3484-4aa1-4385-a48d-4c4fdd5543b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ones_target(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    return data\n",
    "\n",
    "def zeros_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "042a81f7-1487-40be-b409-838efaa355d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function for the discriminator by obtaining the total mini batch loss\n",
    "# in real life senerio, we will calculate the gradients separaely and then update them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "194f1677-5b18-45a7-b9dd-44248a7b83ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer, real_data, fake_data):\n",
    "    N = real_data.size(0)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1.1 Train on Real Data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_real = loss(prediction_real, ones_target(N) )\n",
    "    error_real.backward()\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_fake = loss(prediction_fake, zeros_target(N))\n",
    "    error_fake.backward()\n",
    "    \n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error and predictions for real and fake inputs\n",
    "    return error_real + error_fake, prediction_real, prediction_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "700ca467-c4a7-40e3-8bcd-35062d78925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function for the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f6ee63a-770e-448c-abea-b742ef8f637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(optimizer, fake_data):\n",
    "    N = fake_data.size(0)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Sample noise and generate fake data\n",
    "    prediction = discriminator(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error = loss(prediction, ones_target(N))\n",
    "    error.backward()\n",
    "    # Update weights with gradients\n",
    "    optimizer.step()\n",
    "    # Return error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1363a346-1819-4c3a-b4a4-08b8ba20f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize the training process, we create a static batch of noise and every few steps we will visualize the batch of images the generator outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fad856b9-f15e-4338-a332-882b7e595d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 16\n",
    "test_noise = noise(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d4bb86a-98e5-4556-95f5-224e2da1e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the training with using all the functions we wrote so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a67f9c-db5f-4417-a562-9c339c5acff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger instance\n",
    "logger = Logger(model_name='VGAN', data_name='MNIST')\n",
    "# Total number of epochs to train\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batch, (real_batch,_) in enumerate(data_loader):\n",
    "        N = real_batch.size(0)\n",
    "        # 1. Train Discriminator\n",
    "        real_data = Variable(images_to_vectors(real_batch))\n",
    "        # Generate fake data and detach \n",
    "        # (so gradients are not calculated for generator)\n",
    "        fake_data = generator(noise(N)).detach()\n",
    "        # Train D\n",
    "        d_error, d_pred_real, d_pred_fake = \\\n",
    "              train_discriminator(d_optimizer, real_data, fake_data)\n",
    "\n",
    "        # 2. Train Generator\n",
    "        # Generate fake data\n",
    "        fake_data = generator(noise(N))\n",
    "        # Train G\n",
    "        g_error = train_generator(g_optimizer, fake_data)\n",
    "        # Log batch error\n",
    "        logger.log(d_error, g_error, epoch, n_batch, num_batches)\n",
    "        # Display Progress every few batches\n",
    "        if (n_batch) % 100 == 0: \n",
    "            test_images = vectors_to_images(generator(test_noise))\n",
    "            test_images = test_images.data\n",
    "            logger.log_images(\n",
    "                test_images, num_test_samples, \n",
    "                epoch, n_batch, num_batches\n",
    "            );\n",
    "            # Display status Logs\n",
    "            logger.display_status(\n",
    "                epoch, num_epochs, n_batch, num_batches,\n",
    "                d_error, g_error, d_pred_real, d_pred_fake\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052303f-67ad-4470-ae27-a7f7b0bdf9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it appears the kernel is dying and I cannot resolve this issue and make the code work.\n",
    "# might be the logger that caused it. \n",
    "# running the notebook without the logger, causes the code to freeze.\n",
    "# I have copied the code to a python file where it worked\n",
    "# the Logger did not simulate the training process as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b74a0d-8495-4793-9eea-296f73400c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc207cc-8d70-4e2d-b7de-010a04d32484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58e782-8708-49dd-95e2-4e4030e5bcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
